import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

DATA_PATH = "data/Android_Malware_clean.csv"
LABEL_COL = "Label"

# Model
class SimpleNN(nn.Module):
    def __init__(self, in_features, m_classes):
        super(SimpleNN, self).__init__()
        self.in_features = in_features
        self.fc1 = nn.Linear(self.in_features, 3)
        self.fc2 = nn.Linear(3, 4)
        self.fc3 = nn.Linear(4, 5)
        self.fc4 = nn.Linear(5, m_classes)   # m classes
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.relu(self.fc3(x))
        x = self.fc4(x)   # logits
        return x

def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)

    # 1) Load
    df = pd.read_csv(DATA_PATH, low_memory=False)
    if LABEL_COL not in df.columns:
        raise ValueError(f"Missing label column '{LABEL_COL}'")

    # 2) Split X/y
    X = df.drop(columns=[LABEL_COL])
    y = df[LABEL_COL]

    # 3) Encode labels -> 0..m-1 (required for CrossEntropyLoss)
    le = LabelEncoder()
    y_enc = le.fit_transform(y)
    m = len(le.classes_)
    print("Classes:", list(le.classes_))
    print("m =", m)

    # 4) Train/test split (stratify keeps class balance)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_enc, test_size=0.2, random_state=42, stratify=y_enc
    )

    # 5) Scale features (important for NN stability)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    # 6) Convert to torch tensors
    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    y_train_t = torch.tensor(y_train, dtype=torch.long)
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    y_test_t = torch.tensor(y_test, dtype=torch.long)

    # 7) DataLoaders
    batch_size = 512
    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)

    # 8) Model + loss + optimizer
    model = SimpleNN(in_features=X_train_t.shape[1], m_classes=m).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

    # 9) Train loop
    epochs = 10
    for epoch in range(1, epochs + 1):
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0

        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)

            optimizer.zero_grad()
            logits = model(xb)
            loss = criterion(logits, yb)
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * xb.size(0)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == yb).sum().item()
            total += xb.size(0)

        train_loss = total_loss / total
        train_acc = correct / total

        # 10) Eval
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for xb, yb in test_loader:
                xb, yb = xb.to(device), yb.to(device)
                logits = model(xb)
                preds = torch.argmax(logits, dim=1)
                correct += (preds == yb).sum().item()
                total += xb.size(0)
        test_acc = correct / total

        print(f"Epoch {epoch:02d} | train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | test_acc={test_acc:.4f}")

    # 11) Save model 
    torch.save(
        {"state_dict": model.state_dict(), "label_classes": le.classes_.tolist()},
        "simplenn_android_malware.pt"
    )
    print("Saved model to simplenn_android_malware.pt")

if __name__ == "__main__":
    main()
